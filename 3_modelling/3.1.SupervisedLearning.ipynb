{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning\n",
    "\n",
    "Supervised learning covers models that learn from data labelled with the desired solution by a knowledgable external entitiy. In this notebook, the basic concepts and algorithms for supervised learning will be reviewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add local modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "To learn a model needs to optimize a `cost or loss function` (e.g. **MSE** for regression, **Cross Entropy** `H(p, q)= −Σp(x)log(q(x))` for classification). This optimization is done using Gradient Descent, which finds an optimal (normally a minimum) value by measuring the cost function and its gradient every iteration and moving in the descending direction with a `learning_rate` size step. If this size is too small, the optimization will be slower and if it's too big, it will lead to an unstable process that may not converge to an optimal value.\n",
    "\n",
    "Depending on the data used to iterate, there are three different versions.\n",
    "- **Stochastic (SGD)**: it takes one sample every step, it's fast but more unstable   \n",
    "- **Batch**: it takes the whole training set, it's slow and can lead to overfitting \n",
    "- **Mini-Batch**: it takes a random sample every iteration, it's a balanced between SGD and Batch GD. It performs better in GPUs than SGD, as they are optimized for matrix operations."
   ]
  },
  {
   "source": [
    "To remember how gradient descend works, we will build a linear regression model in PyTorch. First, we will use the SGD optimizers from the library and then we will integrate the loss function *manually*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 0 with loss 12.192981719970703:\n",
      "Y = 0.19980499148368835*x + -1.1746792793273926\n",
      "Iteration 100 with loss 0.010363358072936535:\n",
      "Y = 1.977488398551941*x + 1.010973334312439\n",
      "Iteration 200 with loss 0.010363351553678513:\n",
      "Y = 1.9774489402770996*x + 1.0110245943069458\n",
      "Iteration 300 with loss 0.010363351553678513:\n",
      "Y = 1.9774489402770996*x + 1.0110245943069458\n",
      "Iteration 400 with loss 0.010363351553678513:\n",
      "Y = 1.9774489402770996*x + 1.0110245943069458\n",
      "Iteration 500 with loss 0.010363351553678513:\n",
      "Y = 1.9774489402770996*x + 1.0110245943069458\n",
      "Iteration 600 with loss 0.010363351553678513:\n",
      "Y = 1.9774489402770996*x + 1.0110245943069458\n",
      "Iteration 700 with loss 0.010363351553678513:\n",
      "Y = 1.9774489402770996*x + 1.0110245943069458\n",
      "Iteration 800 with loss 0.010363351553678513:\n",
      "Y = 1.9774489402770996*x + 1.0110245943069458\n",
      "Iteration 900 with loss 0.010363351553678513:\n",
      "Y = 1.9774489402770996*x + 1.0110245943069458\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "N = 64\n",
    "A = 2\n",
    "b = 1\n",
    "error = 0.1\n",
    "\n",
    "x = Variable(torch.randn(N, 1), requires_grad=False)\n",
    "y = A*x + b + Variable(torch.randn(N, 1) * error)\n",
    "\n",
    "A = Variable(torch.randn(1, 1), requires_grad=True)\n",
    "B = Variable(torch.randn(1), requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-2\n",
    "\n",
    "def loss_MSE(predictions, y):\n",
    "    # same as using nn.MSELoss\n",
    "    squared_diffs = (predictions - y)**2\n",
    "    return squared_diffs.mean()\n",
    "\n",
    "optimizer = optim.SGD([A, B], lr=learning_rate)\n",
    "\n",
    "for t in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = torch.matmul(x, A) + B\n",
    "    loss = loss_MSE(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if t%100 == 0:\n",
    "        print(f'Iteration {t} with loss {loss}:')\n",
    "        print(f'Y = {A[0, 0]}*x + {B[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 0 with loss 2.614588737487793:\nY = 0.4222372770309448*x + 0.24920809268951416\nIteration 100 with loss 0.008230608887970448:\nY = 1.9936408996582031*x + 0.9955790638923645\nIteration 200 with loss 0.008229944854974747:\nY = 1.9943785667419434*x + 0.9960685968399048\nIteration 300 with loss 0.008229944854974747:\nY = 1.9943785667419434*x + 0.9960685968399048\nIteration 400 with loss 0.008229944854974747:\nY = 1.9943785667419434*x + 0.9960685968399048\nIteration 500 with loss 0.008229944854974747:\nY = 1.9943785667419434*x + 0.9960685968399048\nIteration 600 with loss 0.008229944854974747:\nY = 1.9943785667419434*x + 0.9960685968399048\nIteration 700 with loss 0.008229944854974747:\nY = 1.9943785667419434*x + 0.9960685968399048\nIteration 800 with loss 0.008229944854974747:\nY = 1.9943785667419434*x + 0.9960685968399048\nIteration 900 with loss 0.008229944854974747:\nY = 1.9943785667419434*x + 0.9960685968399048\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "N = 64\n",
    "A = 2\n",
    "b = 1\n",
    "error = 0.1\n",
    "\n",
    "x = Variable(torch.randn(N, 1), requires_grad=False)\n",
    "y = A*x + b + Variable(torch.randn(N, 1) * error)\n",
    "\n",
    "A = Variable(torch.randn(1, 1), requires_grad=True)\n",
    "B = Variable(torch.randn(1), requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-2\n",
    "\n",
    "def GD(x, y, predictions, A, B):\n",
    "    # Derivate of MSE = (predictions - y)**2/N \n",
    "    # dMSE = 2*(predictions - y)/N\n",
    "    dloss = 2*(predictions - y)/N\n",
    "    dloss_dw = dloss * x\n",
    "    dloss_db = dloss\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])\n",
    "\n",
    "def loss_MSE(predictions, y):\n",
    "    # same as using nn.MSELoss\n",
    "    squared_diffs = (predictions - y)**2\n",
    "    return squared_diffs.mean()\n",
    "\n",
    "for t in range(1000):\n",
    "    y_pred = torch.matmul(x, A) + B\n",
    "    loss = loss_MSE(y_pred, y)\n",
    "    gradients = GD(x, y, y_pred, A, B)\n",
    "    A = A - learning_rate * gradients[0]\n",
    "    B = B - learning_rate * gradients[1]\n",
    "    if t%100 == 0:\n",
    "        print(f'Iteration {t} with loss {loss}:')\n",
    "        print(f'Y = {A[0, 0]}*x + {B[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models\n",
    "\n",
    "Linear models intent to find a predicted value $\\hat{y}$ as a linear combination of weights `w` and features `x`.\n",
    "\n",
    "The simple linear model is the Ordinary Least Squares [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression), which with regularization parameters in the loss function becomes: \n",
    "\n",
    "- [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Lasso): L1 regularization which performs feature selection\n",
    "- [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge): L2 regularization which reduces the weights of features\n",
    "\n",
    "For classification, the most used, simple but powerful algortihm is the [Logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) or `Logit regression` as it uses the logit or `sigmoid function`. \n",
    "\n",
    "For `Multinomial Logistic Regression`, the `Softmax` is applied. With a higher order polynomial in the hypothesis function, it can classifiy non-linear data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Y = 1.9980731092605344*x + 1.0558991996785314\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "N = 64\n",
    "A = 2\n",
    "b = 1\n",
    "error = 0.1\n",
    "\n",
    "x = np.random.rand(N, 1)\n",
    "y = A*x + b + np.random.rand(N, 1) * error\n",
    "\n",
    "model = LinearRegression().fit(x, y)\n",
    "print(f'Y = {model.coef_[0, 0]}*x + {model.intercept_[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "\n",
    "This method tries to find an optimal\n",
    "hyperplane that maximizes the margin between data from different classes. The margin is a `Soft Margin` found by minimizing missclassified datapoints. SVM are very powerful when used with kernels (`polynomial`, `Radial Basis Function (RBF)`, `sigmoid tanh`) to classify non-linear datasets getting complicated decision functions. For this, SVM applies the `Kernel Trick`, which calculates the relation of the data in higher dimensions without really transforming the data.\n",
    "\n",
    "SVM can be used for classification, regression, and even\n",
    "outlier detection problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Based on the Bayes theorem `p(y|x1,..,xn)=p(y)p(x1,..,xn|y)/p(x)`. It is called *naive* as it assumes features are conditionally independent `p(x1|y, x1,..,xn)=p(x1|y)`. `p(y)` and `p(x1|y),...p(xn|y)` are estimated using Maximum A Posteriori (MAP). MAP estimates a distribution of the data based on the observed values.\n",
    "\n",
    "Depending on the data distribution, different decision rules can be applied. It performs well for document classification and spam filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decission Trees\n",
    "\n",
    "Non-parametrics methods used for classification and regressian. Trees can be visualized, work well with numerical and categorical data, but tends to overfit. Thanks to their simplicity, they are used for ensambling methods. \n",
    "One of the most commin decission trees algorithm is the **Classification And Regression Tree (CART)**. It splits the training set in two subsets using a feature `k` and a threshold `tk` so that the data in each group is as pure as possible. \n",
    "\n",
    "To measure the quality, it can use:\n",
    "- `Gini` -> `1 − Σp(i,k)^2` for each class `k` in the node `i` \n",
    "- `Entropy`-> `−Σp(i,k)*log(p(i,k))` for each class `k` in the node `i`\n",
    "\n",
    "Both are zero if all datapoints belong to the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensambling learning\n",
    "\n",
    "### [Bagging: Bootstrap AGGregatING](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier)\n",
    "\n",
    "Generates multiple training datasets using random sampling with replacement and trains in parallel different models which will vote to do a final prediction. Bagging helps to avoid overfitting.\n",
    "\n",
    "[Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) is a bagging method that randomized the features creating more diverse trees.\n",
    "\n",
    "\n",
    "### Boosting\n",
    "\n",
    "It starts creating a simple model that can get good results for easy samples, and it gives bigger weights to samples that are difficult to classify. In the next iteration, it will create models focusing on those samples with higher weights. In that way, it optimizes to get higher accuracies. \n",
    "\n",
    "[AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) is the classical example of Boosting. AdaBoost predicts the class based on weighted votes, as each estimator (tree) has a weight depending on its accuracy. [Gradient Tree Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) is similar, but uses the residuals errors from the last predictor and supports subsampling or `Stochastic Gradient Boosting`. With Stochastic Gradient Boosting a subsample, whose size is controlled with a parameter `subsample`, is taken to train each tree allowing `out-of-bag error` calcualtion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Tuning or Hyperparameter Optimization (HPO)\n",
    "\n",
    "Find the parameters of the model that best fit the data maximazing a defined metric. It helps also to find in which moment the model will start overfitting, by comparing the performance between the `training` and `validation` set.\n",
    "\n",
    "It is normally perfomed using **Cross-Validation** which requires the data being split in three parts: `training`, `test` and `validation`, using the last one to optimize parameters.\n",
    "If data is scarce, **K-fold Cross-Validation** is recommended. It randomly splits the training dataset into K folds, then it trains the model `K` times with the data in `K-1` folds and evaluate in the one left.\n",
    "\n",
    "## Grid-Search\n",
    "\n",
    "`GridSearchCV` in scikit-learn.\n",
    "\n",
    "Evaluation of different combination of hyperparameters values. Results can be observed with heatmaps or 3D scatter plots where the selected metrics are plotted against the hyperparameters values.\n",
    "\n",
    "## Randomized Search\n",
    "\n",
    "`RandomizedSearchCV` in scikit-learn.\n",
    "\n",
    "Evaluates n iterations with random hyperparameters values. To find the parameters that affect more the perfomance, an ANOVA can show the ones which explain more the performance variance.\n",
    "\n",
    "## Bayesian Search\n",
    "\n",
    "It considers past results to create a probabilistic model `p(metric|hyperparameter)` that is used to select the most promising values to be tested in the next iteration. It follos a Bayesian approach, where a more accurate distribution is found with more data. With a good representation, it can in the end choose the optimal value for each hyperparameter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "## Regression\n",
    "\n",
    "- Root-mean-square error (RMSE): perferred metric for regression problems as it penalizes higher errors even though this makes it more sensitive to outliers. It's a non-negative value, a value of zero indicates a perfect fit\n",
    "- Mean Absolute Error (MAE): average of absolute errors\n",
    "- Mean absolute percentage error (MAPE) or mean absolute percentage deviation (MAPD): average of absolute differences (actual - predictions) divided by the actual values\n",
    "\n",
    "\n",
    "## Classification\n",
    "\n",
    "### Confusion matrix\n",
    "\n",
    "Matrix with predictions (rows) vs actual values (columns). **False Negatives (FN)** indicates the probability of **type II error** (Fail to accept a true hypothesis) and **False Positives (FP)** of **type I error** (Fail to reject a false hypothesis).  From there, we can infer important metrics:\n",
    "\n",
    "- Recall or Sensitivity or True Positive Rate (TPR): It's the probability of detection -> `TP/(TP + FN)`\n",
    "- Precision: It's the rate of positives among the predicted as positives -> `TP/(TP + FP)`\n",
    "- F1: It's the harmonic mean of Recall and Precision -> `2*Precision*Recall/(Precision + Recall)`\n",
    "- Specificity: It's the probability of detect negatives -> `TN/(TN + FP)` = 1 - FPR (False Positive rate) -> `FP/(FP + TN)`\n",
    "\n",
    "\n",
    "### Receiver operatir ROC \n",
    "\n",
    "Measures the ability of a binary classifier. It's calculated estimating the **TPR** (Y-axis) and **FPR** (X-axis) for different discrimination thresholds. The **Area Under the Curve (AUC)** of the ROC is the probability of classifying a randomly chosen positive instance higher than a randomly chosen negative one.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems\n",
    " \n",
    "Systemas created to predict users interests based on their previous behavior (e.g. purchase history, ratings, reviews, browsing history) to recommend other products to buy or help them to search. For new users or products, these systems will need some input data (users interests, items details) and a set of rules to start giving recommendations.\n",
    "\n",
    "- **Content-based systems**: or `personality-based approach` based on characteristic information of the products.\n",
    "- **Collaborative filtering systems**: use user-item interactions, resulting in recommendation of that type `users who bought X also bought Y`. They can be `memory-based` (clusters of users or items) or `model-based` (e.g. Item2Vec). As large data is required for them to work, they suffer of the `cold start` problem.\n",
    "- **Hybrid systems**: they combine content and interactions to overcome the shortcomings of each type\n",
    "\n",
    "To measure their performance, MSE, precision or recall can be used. However, online evaluation is needed for models in production. For that, `A/B tesing` comparing `conversion rate` or `click-through rate` is recommended.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "- GÉRON, Aurélien. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O'Reilly Media, 2019.\n",
    "- CHOLLET, Francois. Deep Learning with Python and Keras. Manning Publications, 2018.\n",
    "- [Scikit-Learn User Guide](https://scikit-learn.org/stable/user_guide.html)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit",
   "display_name": "Python 3.7.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "587cef308a2e83ee70ff1385638391f298518764c403ce017a5f243e63a82020"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
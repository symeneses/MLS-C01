{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning\n",
    "\n",
    "Learning from data labelled by a knowledgable external entitiy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decission Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensambling learning\n",
    "\n",
    "### Bagging\n",
    "\n",
    "Generates multiple training datasets using random sampling with replacement and trains in parallel different models which will vote to do a final prediction. Bagging helps to avoid overfitting.\n",
    "\n",
    "\n",
    "### Boosting\n",
    "\n",
    "It starts creating a simple model that can get good results for easy samples, and it gives bigger weights to samples that are difficult to classify. In the next iteration, it will create models focusing on those samples with higher weights. In that way, it optimizes to get higher accuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "### Confusion matrix\n",
    "\n",
    "Matrix with predictions (rows) vs actual values (columns). **False Negatives (FN)** indicates the probability of **type II error** (Fail to accept a true hypothesis) and **False Positives (FP)** of **type I error** (Fail to reject a false hypothesis).  From there, we can infer important metrics:\n",
    "\n",
    "- Recall or Sensitivity or True Positive Rate (TPR): It's the probability of detection -> `TP/(TP + FN)`\n",
    "- Precision: It's the rate of positives among the predicted as positives -> `TP/(TP + FP)`\n",
    "- Specificity: It's the probability of detect negatives -> `TN/(TN + FP)` = 1 - FPR (False Positive rate) -> `FP/(FP + TN)`\n",
    "\n",
    "\n",
    "### Receiver operatir ROC \n",
    "\n",
    "Measures the ability of a binary classifier. It's calculated estimating the **TPR** (Y-axis) and **FPR** (X-axis) for different discrimination thresholds. The **Area Under the Curve (AUC)** of the ROC is the probability of classifying a randomly chosen positive instance higher than a randomly chosen negative one.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "- GÉRON, Aurélien. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O'Reilly Media, 2019.\n",
    "- CHOLLET, Francois. Deep Learning with Python and Keras. Manning Publications, 2018.\n",
    "- SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. Second Edition. MIT Press, 2018."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "It is based on the nature of learning. We learn through **interactions** with our environment, inferring causes and effects of our actions. To talk abour RL, we need to clarify these concepts:\n",
    "\n",
    "- Agent: a goal-seeking learner capable of sense its environment and take actions\n",
    "- Environment: uncertain system that interacts with the agent\n",
    "- Observations: information or feedback that the environment gives to the learner\n",
    "- Actions: etep decided by the agent that influence the future state of the environment, the way the agent behaving is its **policy**.\n",
    "- Reward signal: signal sent by the environment after every step and that agent seeks to maximize over the long run, the long term amount is called **Value function**. A *value* can be delayed and therefore requires planning.\n",
    "\n",
    "Then, we can define RL is the field of ML that seeks to enable an agent to take actions that maximizes its rewards based on observations of its environment.\n",
    "\n",
    "RL produced its first application in the 1950s, with models to play games like backgammon and then forgotten for a while. In 2013, the company DeepMind published \"Playing Atari with Deep Learning\" and it reached its goal of conquering Go, when AlphaGo defeated the world champion in 2017.\n",
    "\n",
    "## Policy search\n",
    "\n",
    "**Policy**  is the algorithm deterministic or stochastic that the *agent* uses to determine its *actions* per perceived state. Then, policy search is the the process to find the policy, among the techniques to find it are: genetic algorithms, optimization techniques (i.e. Policy Gradients (PG)), Monte Carlo tree search and Markov Decision Process (MDP). \n",
    "\n",
    "### Markov Decision Process (MDP)\n",
    "\n",
    "Based on Markov Chains, when the agent updates in each step the probabilities to take a set of actions *a* when it's in the state *s*, optimizing the *value*. In this way, the agent can evaluate the policy. Now, to take the right choice it needs to estimate the optimal *state-action values*, defined as **Q-Values**, the **Q-Value Iteration Algortihm** is applied. This algorithm calculates the sum of discounted future rewards expected after the agent takes the action *a* without knowing the autcome of that action.\n",
    "Unfortunately, the agent doesn't know in the beggining neither the transitions probabilities nor the rewars for each possible transition and action, making MDP not efficient. \n",
    "\n",
    "### Q-Learning\n",
    "\n",
    "It's an specific implementation to learn and optimal policy where the agent explores randomly states to estimate their values with observed rewards. As it acts randomly a parameter **$\\epsilon$-greedy policy** is needed to balance exploration vs explotation, so that, the model will act randomly with probability $\\epsilon$ and will take act based on the highest Q-Value with probability 1-$\\epsilon$. \n",
    "\n",
    "## Model-based methods\n",
    "\n",
    "Methods to infer the environment's behaviour, which will help the agent to *plan*. RL systems can learn by trial and error and or by planning using a *model*.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "- GÉRON, Aurélien. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O'Reilly Media, 2019.\n",
    "- CHOLLET, Francois. Deep Learning with Python and Keras. Manning Publications, 2018.\n",
    "- SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. Second Edition. MIT Press, 2018."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Principles\n",
    "\n",
    "Artificial Neural Networks were defined in 1943 by the neurophysiologist Warren McCulloch and the mathematician Walter Pitts. After few successes (1957 Perceptron by Frank Rosenblatt), they were forgotten until the 80's when new architectures and training techniques were developed. \n",
    "\n",
    "In 1988, D. E. Rumelhart published his *backpropagation* paper \"Learning Internal Representations by Error Propagation\" and Yann Le Cun showed its power with LeNet, identify handwritten numbers for the US post service. However, in the 90's Machine Learning algorithms as SVM were preferred by researchers because of its performance and explainability. Backpropagation, among other training techniques, large datasets, and the increase in computer power (GPUs, CUDA 2007, TPUs 2016) made possible what we know now as deep learning.\n",
    "\n",
    "Deep learning is a field of Machine Learning that focuses on learning successive layers of representations. In this notebook, we will introduce some basic concepts needed to understand deep learning architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight-initialization schemes\n",
    "\n",
    "In backpropagation, as the errors propagates throught the layer, values are getting smaller and smaller impeding training in the lower layers. This is known as **vanishing gradient problem** and the oppossite **exploding gradients problem**. In 2010, Glorot and Bengio gave light to understand the reasons of these problems: *Variance of the outputs are bigger than the inputs*. They proposed  a new **inizialization** (Xavier or Glorot), and in the years after, new schemes have been proposed.\n",
    "\n",
    "- Glorot: weights are initialized with normal distribution mean `0` and sd `sqr(2/(fan_in + fan_out))` or unifor between -r and r, where `r=sqr(6/(fan_in + fan_out))`. \n",
    "- He (2015): it uses a truncated normal distribution with mean `0` and `sd = sqr(2/fan_in + fan_out)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Apart from controlling gradients problems, DL got a big boost from fast optimizers which helps gradient descent to converge faster and scape local optima. \n",
    "\n",
    "- Momentum optimization (1964): Use gradients as accelerator not speed.\n",
    "- Nesterov (1983): use the gradients from a further point in the same direction, converging faster.\n",
    "- AdaGrad (2011): adapt the learning rate, faster for steep dimensions and slower near the optimum. It stops before reaching the optimum sometimes. \n",
    "- RMSProp: Like AdaGrad, but adding exponential decay, so only most recent iteratins are considered.\n",
    "- Adam (2015): momentum + RMSProp, exponential decay average of past gradients and their squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "Actionvation functions sum all the inputs and decide the output.\n",
    "In its origins, perceptron used to work with step functions to **activate** a unit, but to use gradient descient to train Multi-Layer Perceptron (MLP) (1986 by Rumelhart et al), it was replaced by the logistic function (a.k.a sigmoid). After many other fucntions have been proved efective.\n",
    "\n",
    "- Hyperbolic tangent: Output from -1 to 1, it creates normalized output as it's centered around 0 helping to control the vanishing/exploding gradients problem.\n",
    "- Rectified Linear Units (ReLU) = `max(0, z)`: Fast to compute, it doesn't saturate at 1 for large input values helping to avoid vanishing.\n",
    "- LeakyReLU = `max(az, z)`: It includes a small slope to avoid death cells.\n",
    "- Parametric leaky ReLU (PReLU): Similar to LeakyReLU, but it learns or adjusts the slope during training.\n",
    "- Exponential Linear Unit (ELU) (2015) =  `α(exp(z) − 1) if z<0`:  It outperformed ReLU variants as it's smoother and reduced training time but it's slower to compute. \n",
    "- Scaled Exponential Linear Unit (SELU) (2017) = `αe^x−α if z≤0`: It can help NN to self-normalize along with the initializer `lecun_normal`\n",
    "- Swish (2017): It is multiplication of the input with the sigmoid function, so it doesn't change abruptly in 0.\n",
    "- Softmax: Used for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Apart from selectin the right weight initializer, optimizer and activation function, it's important to be aware of these training tricks:\n",
    "- **Batch normalization** (2015): It proposes to normalite the input values of the current batch before the activation function of each layer. Along with a right initialization, it helps to reduce gradients problems and reduces the need of regularization.\n",
    "- **Dropout**: It add a probability `p` to a neuron to be off during a training iteration. The ideas is to make the NN more resilient and flexible. A droput rate of 1 means no dropout.\n",
    "- **Gradicent clipping**: Gradients are clipped at a given `threshold` after their calculation during backpropagation.\n",
    "- **Early stopping**: it stops the training if the performance on the validation set is decreasing, useful to avoid overfitting\n",
    "\n",
    "To debug and monitor training, it is useful to perform `gradient checking` which compares the calculated derivates with an approximation value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "- GÉRON, Aurélien. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O'Reilly Media, 2019.\n",
    "- CHOLLET, Francois. Deep Learning with Python and Keras. Manning Publications, 2018.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}